{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true,"mount_file_id":"1wHVpemyuFVdhAP3yfwLtyxRlj_2YW3_X","authorship_tag":"ABX9TyOit/oRt1BX0mJqPlw+ze1x"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# New section"],"metadata":{"id":"4FRquYeZ1fLg"}},{"cell_type":"code","source":["!pip install openpyxl beautifulsoup4 requests"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ar5pOIjrN3E","executionInfo":{"status":"ok","timestamp":1718991533889,"user_tz":-330,"elapsed":6782,"user":{"displayName":"Rekha singh","userId":"03241740167984145332"}},"outputId":"a5c10ae6-8f94-4594-e932-aa7e88923406"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.4)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PR6o1NzZEWCg","executionInfo":{"status":"ok","timestamp":1718994710296,"user_tz":-330,"elapsed":8437,"user":{"displayName":"Rekha singh","userId":"03241740167984145332"}},"outputId":"c674c0c5-92f9-4b28-fe92-26cd0c2dc31b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n","[Errno 2] No such file or directory: '/gdrive/MyDrive/project/Data_Extraction_and_NLP/TestAssignment'\n","/content\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import os\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","import re\n","\n","drive.mount('/content/drive')\n","%cd '/content/drive/MyDrive/20211030 Test Assignment'\n","\n","# reading the url files\n","df = pd.read_excel('/content/drive/MyDrive/20211030 Test Assignment/Input.xlsx')\n","\n","# traverse in loop over the df\n","for index, row in df.iterrows():\n","    url = row['URL']\n","    url_id = row['URL_ID']\n","\n","    #requesting the url\n","    header = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"}\n","    try:\n","        response = requests.get(url, headers=header)\n","    except:\n","        print(\"can't get response of {}\".format(url_id))\n","        continue\n","\n","    #creating a beautifulsoup object\n","    try:\n","        soup = BeautifulSoup(response.content, 'html.parser')\n","    except:\n","        print(\"can't get page of {}\".format(url_id))\n","        continue\n","\n","    #find title from the texts\n","    try:\n","        title = soup.find('h1').get_text()\n","    except:\n","        print(\"can't get title of {}\".format(url_id))\n","        continue\n","\n","    # find texts\n","    article = \"\"\n","    try:\n","        for p in soup.find_all('p'):\n","            article += p.get_text()\n","    except:\n","        print(\"can't get text of {}\".format(url_id))\n","\n","    # write title and text to the file\n","    file_name = '/content/drive/MyDrive/20211030 Test Assignment' + str(url_id) + '.txt'\n","    with open(file_name, 'w') as file:\n","        file.write(title + '\\n' + article)\n","\n","# Directories\n","text_dir = \"/content/drive/MyDrive/20211030 Test Assignment\"\n","stopwords_dir = \"/content/drive/MyDrive/20211030 Test Assignment/StopWords\"\n","sentiment_dir = \"/content/drive/MyDrive/20211030 Test Assignment/MasterDictionary\"\n","\n","# loading all stop words from the stopwords directory and storing in the set variable\n","stop_words = set()\n","for files in os.listdir(stopwords_dir):\n","    with open(os.path.join(stopwords_dir, files), 'r', encoding='ISO-8859-1') as f:\n","        stop_words.update(set(f.read().splitlines()))\n","\n","# loading  all text files from the directory and storing in a list\n","docs = []\n","for text_file in os.listdir(text_dir):\n","    with open(os.path.join(text_dir, text_file), 'r') as f:\n","        text = f.read()\n","        # tokenizing the text file\n","        words = word_tokenize(text)\n","        # remove the stop words from the tokens\n","        filtered_text = [word for word in words if word.lower() not in stop_words]\n","        # add each filtered tokens of each file into a list\n","        docs.append(filtered_text)\n","\n","# store positive, negative words from the directory\n","pos = set()\n","neg = set()\n","\n","for files in os.listdir(sentiment_dir):\n","    if files == 'positive-words.txt':\n","        with open(os.path.join(sentiment_dir, files), 'r', encoding='ISO-8859-1') as f:\n","            pos.update(f.read().splitlines())\n","    else:\n","        with open(os.path.join(sentiment_dir, files), 'r', encoding='ISO-8859-1') as f:\n","            neg.update(f.read().splitlines())\n","\n","# now collect the positive and negative words from each file\n","# calculate the scores from the positive and negative words\n","positive_words = []\n","Negative_words = []\n","positive_score = []\n","negative_score = []\n","polarity_score = []\n","subjectivity_score = []\n","\n","# iterate through the list of docs\n","for i in range(len(docs)):\n","    positive_words.append([word for word in docs[i] if word.lower() in pos])\n","    Negative_words.append([word for word in docs[i] if word.lower() in neg])\n","    positive_score.append(len(positive_words[i]))\n","    negative_score.append(len(Negative_words[i]))\n","    polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))\n","    subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(docs[i])) + 0.000001))\n","\n","# Average Sentence Length = the number of words / the number of sentences\n","# Percentage of Complex words = the number of complex words / the number of words\n","# Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n","\n","avg_sentence_length = []\n","Percentage_of_Complex_words = []\n","Fog_Index = []\n","complex_word_count = []\n","avg_syllable_word_count = []\n","\n","stopwords = set(stopwords.words('english'))\n","\n","def measure(file):\n","    with open(os.path.join(text_dir, file), 'r') as f:\n","        text = f.read()\n","        # remove punctuations\n","        text = re.sub(r'[^\\w\\s.]', '', text)\n","        # split the given text file into sentences\n","        sentences = text.split('.')\n","        # total number of sentences in a file\n","        num_sentences = len(sentences)\n","        # total words in the file\n","        words = [word for word in text.split() if word.lower() not in stopwords]\n","        num_words = len(words)\n","\n","        # complex words having syllable count is greater than 2\n","        # Complex words are words in the text that contain more than two syllables.\n","        complex_words = []\n","        for word in words:\n","            vowels = 'aeiou'\n","            syllable_count_word = sum(1 for letter in word if letter.lower() in vowels)\n","            if syllable_count_word > 2:\n","                complex_words.append(word)\n","\n","        # Syllable Count Per Word\n","        # We count the number of Syllables in each word of the text by counting the vowels present in each word.\n","        # We also handle some exceptions like words ending with \"es\",\"ed\" by not counting them as a syllable.\n","        syllable_count = 0\n","        syllable_words = []\n","        for word in words:\n","            if word.endswith('es'):\n","                word = word[:-2]\n","            elif word.endswith('ed'):\n","                word = word[:-2]\n","            vowels = 'aeiou'\n","            syllable_count_word = sum(1 for letter in word if letter.lower() in vowels)\n","            if syllable_count_word >= 1:\n","                syllable_words.append(word)\n","                syllable_count += syllable_count_word\n","\n","        avg_sentence_len = num_words / num_sentences\n","        avg_syllable_word_count = syllable_count / len(syllable_words)\n","        Percent_Complex_words = len(complex_words) / num_words\n","        Fog_Index = 0.4 * (avg_sentence_len + Percent_Complex_words)\n","\n","        return avg_sentence_len, Percent_Complex_words, Fog_Index, len(complex_words), avg_syllable_word_count\n","\n","# iterate through each file or doc\n","for file in os.listdir(text_dir):\n","    x, y, z, a, b = measure(file)\n","    avg_sentence_length.append(x)\n","    Percentage_of_Complex_words.append(y)\n","    Fog_Index.append(z)\n","    complex_word_count.append(a)\n","    avg_syllable_word_count.append(b)\n","\n","# Word Count and Average Word Length Sum of the total number of characters in each word/Total number of words\n","# We count the total cleaned words present in the text by\n","# removing the stop words (using stopwords class of nltk package).\n","# removing any punctuations like ? ! , . from the word before counting.\n","\n","def cleaned_words(file):\n","    with open(os.path.join(text_dir, file), 'r') as f:\n","        text = f.read()\n","        text = re.sub(r'[^\\w\\s]', '', text)\n","        words = [word for word in text.split() if word.lower() not in stopwords]\n","        length = sum(len(word) for word in words)\n","        average_word_length = length / len(words)\n","    return len(words), average_word_length\n","\n","word_count = []\n","average_word_length = []\n","for file in os.listdir(text_dir):\n","    x, y = cleaned_words(file)\n","    word_count.append(x)\n","    average_word_length.append(y)\n","\n","# To calculate Personal Pronouns mentioned in the text, we use regex to find\n","# the counts of the words - “I,” “we,” “my,” “ours,” and “us”. Special care is taken\n","# so that the country name US is not included in the list.\n","def count_personal_pronouns(file):\n","    with open(os.path.join(text_dir, file), 'r') as f:\n","        text = f.read()\n","        personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n","        count = 0\n","        for pronoun in personal_pronouns:\n","            count += len(re.findall(r\"\\b\" + pronoun + r\"\\b\", text))  # \\b is used to match word boundaries\n","    return count\n","\n","pp_count = []\n","for file in os.listdir(text_dir):\n","    x = count_personal_pronouns(file)\n","    pp_count.append(x)\n","\n","output_df = pd.read_excel('/content/drive/MyDrive/20211030 Test Assignment/Output Data Structure.xlsx')\n","\n","# URL_ID 44, 57, 144 do not exist, i.e., page does not exist, throws 404 error\n","# so we are going to drop these rows from the table\n","output_df.drop([44-37, 57-37, 144-37], axis=0, inplace=True)\n","\n","# These are the required parameters\n","variables = [positive_score,\n","             negative_score,\n","             polarity_score,\n","             subjectivity_score,\n","             avg_sentence_length,\n","             Percentage_of_Complex_words,\n","             Fog_Index,\n","             avg_sentence_length,\n","             complex_word_count,\n","             word_count,\n","             avg_syllable_word_count,\n","             pp_count,\n","             average_word_length]\n","\n","# write the values to the dataframe\n","for i, var in enumerate(variables):\n","    output_df.iloc[:, i+2] = var\n","\n","# now save the dataframe to the disk\n","output_df.to_csv('/content/drive/MyDrive/20211030 Test Assignment/Output_Data.csv', index=False)\n"],"metadata":{"id":"f2r2aogxDb0O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import re\n","import pandas as pd\n","from nltk.corpus import stopwords as nltk_stopwords\n","from nltk.tokenize import word_tokenize\n","import datetime\n","\n","# Directories\n","text_dir = \"/content/drive/MyDrive/20211030 Test Assignment\"\n","stopwords_dir = \"/content/drive/MyDrive/20211030 Test Assignment/StopWords\"\n","sentiment_dir = \"/content/drive/MyDrive/20211030 Test Assignment/MasterDictionary\"\n","\n","# Load all stop words from the stopwords directory\n","stop_words = set()\n","for file in os.listdir(stopwords_dir):\n","    with open(os.path.join(stopwords_dir, file), 'r', encoding='ISO-8859-1') as f:\n","        stop_words.update(set(f.read().splitlines()))\n","\n","# Load positive and negative words\n","pos = set()\n","neg = set()\n","for file in os.listdir(sentiment_dir):\n","    if file == 'positive-words.txt':\n","        with open(os.path.join(sentiment_dir, file), 'r', encoding='ISO-8859-1') as f:\n","            pos.update(f.read().splitlines())\n","    elif file == 'negative-words.txt':\n","        with open(os.path.join(sentiment_dir, file), 'r', encoding='ISO-8859-1') as f:\n","            neg.update(f.read().splitlines())\n","\n","# Load the output structure\n","output_df = pd.read_excel('/content/drive/MyDrive/20211030 Test Assignment/Output Data Structure.xlsx')\n","\n","# Function to process each text file\n","def process_file(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        text = f.read()\n","\n","    # Tokenize and filter stop words\n","    words = word_tokenize(text)\n","    filtered_words = [word for word in words if word.lower() not in stop_words]\n","\n","    # Calculate scores\n","    pos_words = [word for word in filtered_words if word.lower() in pos]\n","    neg_words = [word for word in filtered_words if word.lower() in neg]\n","    positive_score = len(pos_words)\n","    negative_score = len(neg_words)\n","    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n","    subjectivity_score = (positive_score + negative_score) / (len(filtered_words) + 0.000001)\n","\n","    # Calculate readability metrics\n","    sentences = text.split('.')\n","    num_sentences = len(sentences)\n","    num_words = len([word for word in text.split() if word.lower() not in nltk_stopwords.words('english')])\n","\n","    complex_words = [word for word in filtered_words if sum(1 for letter in word if letter.lower() in 'aeiou') > 2]\n","    avg_sentence_length = num_words / num_sentences\n","    percentage_complex_words = len(complex_words) / num_words\n","    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n","\n","    # Calculate syllable and word metrics\n","    syllable_count = sum(max(1, sum(1 for letter in word if letter.lower() in 'aeiou')) for word in filtered_words)\n","    avg_syllable_word_count = syllable_count / len(filtered_words)\n","\n","    # Calculate word count and average word length\n","    cleaned_words = [word for word in re.sub(r'[^\\w\\s]', '', text).split() if word.lower() not in nltk_stopwords.words('english')]\n","    word_count = len(cleaned_words)\n","    avg_word_length = sum(len(word) for word in cleaned_words) / word_count\n","\n","    # Count personal pronouns\n","    personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n","    pp_count = sum(len(re.findall(r\"\\b\" + pronoun + r\"\\b\", text, re.IGNORECASE)) for pronoun in personal_pronouns)\n","\n","    return [positive_score, negative_score, polarity_score, subjectivity_score,\n","            avg_sentence_length, percentage_complex_words, fog_index,\n","            avg_sentence_length, len(complex_words), word_count,\n","            avg_syllable_word_count, pp_count, avg_word_length]\n","\n","# Process all files\n","results = []\n","for index, row in output_df.iterrows():\n","    file_name = f\"{row['URL_ID']}.txt\"\n","    file_path = os.path.join(text_dir, file_name)\n","    if os.path.exists(file_path):\n","        results.append(process_file(file_path))\n","    else:\n","        results.append([None] * 13)  # Append None values if file doesn't exist\n","\n","# Update the DataFrame\n","for i, col in enumerate(output_df.columns[2:]):\n","    output_df[col] = [row[i] if row else None for row in results]\n","\n","# Save the results using a file stream\n","from io import StringIO\n","\n","# Convert DataFrame to CSV string\n","csv_string = output_df.to_csv(index=False)\n","\n","# Write the CSV string to a file in Google Drive\n","output_dir = '/content/drive/MyDrive/output_articles'\n","timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","output_file = f'Output_Data_{timestamp}.csv'\n","output_path = os.path.join(output_dir, output_file)\n","\n","# Ensure the directory exists\n","os.makedirs(output_dir, exist_ok=True)\n","\n","try:\n","    # Write the file\n","    with open(output_path, 'w', encoding='utf-8') as f:\n","        f.write(csv_string)\n","    print(f\"Processing complete. Results saved to {output_path}\")\n","except IsADirectoryError:\n","    print(f\"Error: {output_path} is a directory. Please choose a different file name.\")\n","except PermissionError:\n","    print(f\"Error: No permission to write to {output_path}. Please check your access rights.\")\n","except Exception as e:\n","    print(f\"An unexpected error occurred: {str(e)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5arHKUBwO9R7","executionInfo":{"status":"ok","timestamp":1719000068076,"user_tz":-330,"elapsed":5222,"user":{"displayName":"Rekha singh","userId":"03241740167984145332"}},"outputId":"c38310e8-075f-4050-d69f-76339020149f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Processing complete. Results saved to /content/drive/MyDrive/output_articles/Output_Data_20240621_200106.csv\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import re\n","import pandas as pd\n","from nltk.corpus import stopwords as nltk_stopwords\n","from nltk.tokenize import word_tokenize\n","import datetime\n","import nltk\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Directories\n","text_dir = \"/content/drive/MyDrive/20211030 Test Assignment/TitleText\"\n","stopwords_dir = \"/content/drive/MyDrive/20211030 Test Assignment/StopWords\"\n","sentiment_dir = \"/content/drive/MyDrive/20211030 Test Assignment/MasterDictionary\"\n","\n","# Load all stop words from the stopwords directory\n","stop_words = set()\n","for file in os.listdir(stopwords_dir):\n","    with open(os.path.join(stopwords_dir, file), 'r', encoding='ISO-8859-1') as f:\n","        stop_words.update(set(f.read().splitlines()))\n","\n","# Load positive and negative words\n","pos = set()\n","neg = set()\n","for file in os.listdir(sentiment_dir):\n","    if file == 'positive-words.txt':\n","        with open(os.path.join(sentiment_dir, file), 'r', encoding='ISO-8859-1') as f:\n","            pos.update(f.read().splitlines())\n","    elif file == 'negative-words.txt':\n","        with open(os.path.join(sentiment_dir, file), 'r', encoding='ISO-8859-1') as f:\n","            neg.update(f.read().splitlines())\n","\n","# Load the output structure\n","output_df = pd.read_excel('/content/drive/MyDrive/20211030 Test Assignment/Output Data Structure.xlsx')\n","\n","# Function to process each text file\n","def process_file(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        text = f.read()\n","\n","    # Tokenize and filter stop words\n","    words = word_tokenize(text)\n","    filtered_words = [word for word in words if word.lower() not in stop_words]\n","\n","    # Calculate scores\n","    pos_words = [word for word in filtered_words if word.lower() in pos]\n","    neg_words = [word for word in filtered_words if word.lower() in neg]\n","    positive_score = len(pos_words)\n","    negative_score = len(neg_words)\n","    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n","    subjectivity_score = (positive_score + negative_score) / (len(filtered_words) + 0.000001)\n","\n","    # Calculate readability metrics\n","    sentences = text.split('.')\n","    num_sentences = len(sentences)\n","    num_words = len([word for word in text.split() if word.lower() not in nltk_stopwords.words('english')])\n","\n","    complex_words = [word for word in filtered_words if sum(1 for letter in word if letter.lower() in 'aeiou') > 2]\n","    avg_sentence_length = num_words / num_sentences\n","    percentage_complex_words = len(complex_words) / num_words\n","    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n","\n","    # Calculate syllable and word metrics\n","    syllable_count = sum(max(1, sum(1 for letter in word if letter.lower() in 'aeiou')) for word in filtered_words)\n","    avg_syllable_word_count = syllable_count / len(filtered_words)\n","\n","    # Calculate word count and average word length\n","    cleaned_words = [word for word in re.sub(r'[^\\w\\s]', '', text).split() if word.lower() not in nltk_stopwords.words('english')]\n","    word_count = len(cleaned_words)\n","    avg_word_length = sum(len(word) for word in cleaned_words) / word_count\n","\n","    # Count personal pronouns\n","    personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n","    pp_count = sum(len(re.findall(r\"\\b\" + pronoun + r\"\\b\", text, re.IGNORECASE)) for pronoun in personal_pronouns)\n","\n","    return [positive_score, negative_score, polarity_score, subjectivity_score,\n","            avg_sentence_length, percentage_complex_words, fog_index,\n","            avg_sentence_length, len(complex_words), word_count,\n","            avg_syllable_word_count, pp_count, avg_word_length]\n","\n","# Process all files\n","results = []\n","for index, row in output_df.iterrows():\n","    file_name = f\"{row['URL_ID']}.txt\"\n","    file_path = os.path.join(text_dir, file_name)\n","    if os.path.exists(file_path):\n","        results.append(process_file(file_path))\n","    else:\n","        results.append([None] * 13)  # Append None values if file doesn't exist\n","\n","# Update the DataFrame\n","for i, col in enumerate(output_df.columns[2:]):\n","    output_df[col] = [row[i] if row else None for row in results]\n","\n","# Save the results using a file stream\n","from io import StringIO\n","\n","# Convert DataFrame to CSV string\n","csv_string = output_df.to_csv(index=False)\n","\n","# Write the CSV string to a file in Google Drive\n","output_dir = '/content/drive/MyDrive/output_articles'\n","os.makedirs(output_dir, exist_ok=True)\n","timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","output_file_path = os.path.join(output_dir, f'Output_Data_{timestamp}.csv')\n","\n","with open(output_file_path, 'w') as f:\n","    f.write(csv_string)\n","\n","print(f\"Output saved to {output_file_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C3dM5UrAcB8e","executionInfo":{"status":"ok","timestamp":1719034762431,"user_tz":-330,"elapsed":29642,"user":{"displayName":"Rekha singh","userId":"03241740167984145332"}},"outputId":"05e1d438-2555-466d-816c-c8850e4e756d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"stream","name":"stdout","text":["Output saved to /content/drive/MyDrive/output_articles/Output_Data_20240622_053920.csv\n"]}]},{"cell_type":"code","source":["import os\n","import pandas as pd\n","from bs4 import BeautifulSoup\n","import requests\n","\n","# Load input Excel file\n","input_df = pd.read_excel('/content/drive/MyDrive/20211030 Test Assignment/Input.xlsx')\n","\n","# Directory to save the extracted text files\n","extracted_text_dir = \"/content/drive/MyDrive/output_articles\"\n","os.makedirs(extracted_text_dir, exist_ok=True)\n","\n","# Function to extract article text from a URL\n","def extract_text_from_url(url):\n","    try:\n","        response = requests.get(url)\n","        if response.status_code == 200:\n","            soup = BeautifulSoup(response.content, 'html.parser')\n","            title = soup.title.string if soup.title else \"\"\n","            paragraphs = soup.find_all('p')\n","            article_text = \" \".join([p.get_text() for p in paragraphs])\n","            return title + \"\\n\" + article_text\n","        else:\n","            return \"\"\n","    except Exception as e:\n","        print(f\"Error extracting {url}: {e}\")\n","        return \"\"\n","\n","# Extract text for each URL and save to a text file\n","for index, row in input_df.iterrows():\n","    url_id = row['URL_ID']\n","    url = row['URL']\n","    article_text = extract_text_from_url(url)\n","    if article_text:\n","        file_path = os.path.join(extracted_text_dir, f\"{url_id}.txt\")\n","        with open(file_path, 'w', encoding='utf-8') as f:\n","            f.write(article_text)\n"],"metadata":{"id":"c5gjP3iqgIln"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import re\n","import pandas as pd\n","from nltk.corpus import stopwords as nltk_stopwords\n","from nltk.tokenize import word_tokenize\n","import datetime\n","import nltk\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Directories\n","text_dir = \"/content/drive/MyDrive/20211030 Test Assignment/TitleText\"\n","stopwords_dir = \"/content/drive/MyDrive/20211030 Test Assignment/StopWords\"\n","sentiment_dir = \"/content/drive/MyDrive/20211030 Test Assignment/MasterDictionary\"\n","\n","# Load all stop words from the stopwords directory\n","stop_words = set()\n","for file in os.listdir(stopwords_dir):\n","    with open(os.path.join(stopwords_dir, file), 'r', encoding='ISO-8859-1') as f:\n","        stop_words.update(set(f.read().splitlines()))\n","\n","# Load positive and negative words\n","pos = set()\n","neg = set()\n","for file in os.listdir(sentiment_dir):\n","    if file == 'positive-words.txt':\n","        with open(os.path.join(sentiment_dir, file), 'r', encoding='ISO-8859-1') as f:\n","            pos.update(f.read().splitlines())\n","    elif file == 'negative-words.txt':\n","        with open(os.path.join(sentiment_dir, file), 'r', encoding='ISO-8859-1') as f:\n","            neg.update(f.read().splitlines())\n","\n","# Load the output structure\n","output_df = pd.read_excel('/content/drive/MyDrive/20211030 Test Assignment/Output Data Structure.xlsx')\n","\n","# Function to process each text file\n","def process_file(file_path):\n","    with open(file_path, 'r', encoding='utf-8') as f:\n","        text = f.read()\n","\n","    # Tokenize and filter stop words\n","    words = word_tokenize(text)\n","    filtered_words = [word for word in words if word.lower() not in stop_words]\n","\n","    # Calculate scores\n","    pos_words = [word for word in filtered_words if word.lower() in pos]\n","    neg_words = [word for word in filtered_words if word.lower() in neg]\n","    positive_score = len(pos_words)\n","    negative_score = len(neg_words)\n","    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n","    subjectivity_score = (positive_score + negative_score) / (len(filtered_words) + 0.000001)\n","\n","    # Calculate readability metrics\n","    sentences = text.split('.')\n","    num_sentences = len(sentences)\n","    num_words = len([word for word in text.split() if word.lower() not in nltk_stopwords.words('english')])\n","\n","    complex_words = [word for word in filtered_words if sum(1 for letter in word if letter.lower() in 'aeiou') > 2]\n","    avg_sentence_length = num_words / num_sentences\n","    percentage_complex_words = len(complex_words) / num_words\n","    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n","\n","    # Calculate syllable and word metrics\n","    syllable_count = sum(max(1, sum(1 for letter in word if letter.lower() in 'aeiou')) for word in filtered_words)\n","    avg_syllable_word_count = syllable_count / len(filtered_words)\n","\n","    # Calculate word count and average word length\n","    cleaned_words = [word for word in re.sub(r'[^\\w\\s]', '', text).split() if word.lower() not in nltk_stopwords.words('english')]\n","    word_count = len(cleaned_words)\n","    avg_word_length = sum(len(word) for word in cleaned_words) / word_count\n","\n","    # Count personal pronouns\n","    personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n","    pp_count = sum(len(re.findall(r\"\\b\" + pronoun + r\"\\b\", text, re.IGNORECASE)) for pronoun in personal_pronouns)\n","\n","    return [positive_score, negative_score, polarity_score, subjectivity_score,\n","            avg_sentence_length, percentage_complex_words, fog_index,\n","            avg_sentence_length, len(complex_words), word_count,\n","            avg_syllable_word_count, pp_count, avg_word_length]\n","\n","# Process all files\n","results = []\n","for index, row in output_df.iterrows():\n","    file_name = f\"{row['URL_ID']}.txt\"\n","    file_path = os.path.join(text_dir, file_name)\n","    if os.path.exists(file_path):\n","        results.append(process_file(file_path))\n","    else:\n","        results.append([None] * 13)  # Append None values if file doesn't exist\n","\n","# Update the DataFrame by starting from the column after 'URL'\n","for i, col in enumerate(output_df.columns[2:]):\n","    output_df[col] = [row[i] if row else None for row in results]\n","\n","# Save the results using a file stream\n","from io import StringIO\n","\n","# Convert DataFrame to CSV string\n","csv_string = output_df.to_csv(index=False)\n","\n","# Write the CSV string to a file in Google Drive\n","output_dir = '/content/drive/MyDrive/output_articles'\n","os.makedirs(output_dir, exist_ok=True)\n","timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","output_file_path = os.path.join(output_dir, f'Output_Data_{timestamp}.csv')\n","\n","with open(output_file_path, 'w') as f:\n","    f.write(csv_string)\n","\n","print(f\"Output saved to {output_file_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SLVzCNP0hbVI","executionInfo":{"status":"ok","timestamp":1719036256407,"user_tz":-330,"elapsed":659,"user":{"displayName":"Rekha singh","userId":"03241740167984145332"}},"outputId":"3f8bc952-462b-4915-c685-d85bccf54f31"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Output saved to /content/drive/MyDrive/output_articles/Output_Data_20240622_060414.csv\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]}]}